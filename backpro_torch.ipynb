{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "METHODS = ['default', 'ror','uniform','nri_uniform','var']\n",
    "DATA_LBP_PATH = 'dataset/fish_data_lbp/resize/'\n",
    "DATA_HOG_PATH = 'dataset/fish_data_hog'\n",
    "DATA_HL_PATH = 'dataset/fish_data_houghline'\n",
    "METHOD_INDEX = 4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4755</th>\n",
       "      <th>4756</th>\n",
       "      <th>4757</th>\n",
       "      <th>4758</th>\n",
       "      <th>4759</th>\n",
       "      <th>4760</th>\n",
       "      <th>4761</th>\n",
       "      <th>4762</th>\n",
       "      <th>4763</th>\n",
       "      <th>4764</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>1</td>\n",
       "      <td>0.039008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027035</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402458</td>\n",
       "      <td>0.252968</td>\n",
       "      <td>0.402458</td>\n",
       "      <td>0.301150</td>\n",
       "      <td>0.402458</td>\n",
       "      <td>0.036988</td>\n",
       "      <td>0.329734</td>\n",
       "      <td>0.295521</td>\n",
       "      <td>0.402458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>3</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>4</td>\n",
       "      <td>0.035968</td>\n",
       "      <td>0.414215</td>\n",
       "      <td>0.414215</td>\n",
       "      <td>0.414215</td>\n",
       "      <td>0.351986</td>\n",
       "      <td>0.414215</td>\n",
       "      <td>0.120783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082304</td>\n",
       "      <td>0.432131</td>\n",
       "      <td>0.430995</td>\n",
       "      <td>0.432131</td>\n",
       "      <td>0.112794</td>\n",
       "      <td>0.432131</td>\n",
       "      <td>0.186883</td>\n",
       "      <td>0.432131</td>\n",
       "      <td>0.046721</td>\n",
       "      <td>0.132146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>5</td>\n",
       "      <td>0.027966</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>6</td>\n",
       "      <td>0.032001</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>0.125557</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>0.125557</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.526888</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.456013</td>\n",
       "      <td>0.120170</td>\n",
       "      <td>0.526888</td>\n",
       "      <td>0.120170</td>\n",
       "      <td>0.456013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>A73EGS-P</td>\n",
       "      <td>7</td>\n",
       "      <td>0.035001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>acanthaluteres_brownii</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.446425</td>\n",
       "      <td>0.411431</td>\n",
       "      <td>0.446425</td>\n",
       "      <td>0.235103</td>\n",
       "      <td>0.446425</td>\n",
       "      <td>0.083122</td>\n",
       "      <td>...</td>\n",
       "      <td>0.386499</td>\n",
       "      <td>0.376669</td>\n",
       "      <td>0.144562</td>\n",
       "      <td>0.327824</td>\n",
       "      <td>0.376669</td>\n",
       "      <td>0.156876</td>\n",
       "      <td>0.376669</td>\n",
       "      <td>0.370978</td>\n",
       "      <td>0.376669</td>\n",
       "      <td>0.376669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>acanthaluteres_brownii</td>\n",
       "      <td>2</td>\n",
       "      <td>0.033000</td>\n",
       "      <td>0.444278</td>\n",
       "      <td>0.316934</td>\n",
       "      <td>0.392861</td>\n",
       "      <td>0.056470</td>\n",
       "      <td>0.444278</td>\n",
       "      <td>0.112940</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.601388</td>\n",
       "      <td>0.199759</td>\n",
       "      <td>0.084226</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.238227</td>\n",
       "      <td>0.066586</td>\n",
       "      <td>0.601388</td>\n",
       "      <td>0.399518</td>\n",
       "      <td>0.094167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>acanthaluteres_brownii</td>\n",
       "      <td>3</td>\n",
       "      <td>0.029996</td>\n",
       "      <td>0.342638</td>\n",
       "      <td>0.342638</td>\n",
       "      <td>0.246561</td>\n",
       "      <td>0.342638</td>\n",
       "      <td>0.342638</td>\n",
       "      <td>0.342638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>acanthaluteres_brownii</td>\n",
       "      <td>4</td>\n",
       "      <td>0.033960</td>\n",
       "      <td>0.528693</td>\n",
       "      <td>0.528693</td>\n",
       "      <td>0.166346</td>\n",
       "      <td>0.050066</td>\n",
       "      <td>0.191052</td>\n",
       "      <td>0.060581</td>\n",
       "      <td>...</td>\n",
       "      <td>0.338371</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.501302</td>\n",
       "      <td>0.282239</td>\n",
       "      <td>0.127177</td>\n",
       "      <td>0.158951</td>\n",
       "      <td>0.064110</td>\n",
       "      <td>0.319059</td>\n",
       "      <td>0.138134</td>\n",
       "      <td>0.501302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>acanthaluteres_spilomelanurus</td>\n",
       "      <td>1</td>\n",
       "      <td>0.025006</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>acanthaluteres_spilomelanurus</td>\n",
       "      <td>2</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.657971</td>\n",
       "      <td>0.064249</td>\n",
       "      <td>0.016557</td>\n",
       "      <td>0.024136</td>\n",
       "      <td>0.137567</td>\n",
       "      <td>0.025158</td>\n",
       "      <td>...</td>\n",
       "      <td>0.687689</td>\n",
       "      <td>0.350994</td>\n",
       "      <td>0.350994</td>\n",
       "      <td>0.350994</td>\n",
       "      <td>0.350994</td>\n",
       "      <td>0.250163</td>\n",
       "      <td>0.273932</td>\n",
       "      <td>0.350994</td>\n",
       "      <td>0.350994</td>\n",
       "      <td>0.350994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>acanthaluteres_spilomelanurus</td>\n",
       "      <td>3</td>\n",
       "      <td>0.030999</td>\n",
       "      <td>0.525479</td>\n",
       "      <td>0.211525</td>\n",
       "      <td>0.267561</td>\n",
       "      <td>0.072952</td>\n",
       "      <td>0.310819</td>\n",
       "      <td>0.090654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689251</td>\n",
       "      <td>0.510947</td>\n",
       "      <td>0.470526</td>\n",
       "      <td>0.101826</td>\n",
       "      <td>0.161002</td>\n",
       "      <td>0.187206</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.183288</td>\n",
       "      <td>0.510947</td>\n",
       "      <td>0.387949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>1</td>\n",
       "      <td>0.026039</td>\n",
       "      <td>0.427143</td>\n",
       "      <td>0.427143</td>\n",
       "      <td>0.427143</td>\n",
       "      <td>0.235712</td>\n",
       "      <td>0.251186</td>\n",
       "      <td>0.131000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.563029</td>\n",
       "      <td>0.560311</td>\n",
       "      <td>0.432229</td>\n",
       "      <td>0.130764</td>\n",
       "      <td>0.038116</td>\n",
       "      <td>0.020357</td>\n",
       "      <td>0.016429</td>\n",
       "      <td>0.010390</td>\n",
       "      <td>0.407353</td>\n",
       "      <td>0.560311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>2</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>3</td>\n",
       "      <td>0.034005</td>\n",
       "      <td>0.493773</td>\n",
       "      <td>0.451036</td>\n",
       "      <td>0.240283</td>\n",
       "      <td>0.084106</td>\n",
       "      <td>0.180309</td>\n",
       "      <td>0.068568</td>\n",
       "      <td>...</td>\n",
       "      <td>0.148360</td>\n",
       "      <td>0.502288</td>\n",
       "      <td>0.286801</td>\n",
       "      <td>0.129382</td>\n",
       "      <td>0.239781</td>\n",
       "      <td>0.102359</td>\n",
       "      <td>0.107700</td>\n",
       "      <td>0.254075</td>\n",
       "      <td>0.502288</td>\n",
       "      <td>0.502288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>4</td>\n",
       "      <td>0.030978</td>\n",
       "      <td>0.361551</td>\n",
       "      <td>0.344566</td>\n",
       "      <td>0.240462</td>\n",
       "      <td>0.197831</td>\n",
       "      <td>0.361551</td>\n",
       "      <td>0.361551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.385947</td>\n",
       "      <td>0.412755</td>\n",
       "      <td>0.412755</td>\n",
       "      <td>0.292546</td>\n",
       "      <td>0.153693</td>\n",
       "      <td>0.134140</td>\n",
       "      <td>0.190870</td>\n",
       "      <td>0.393578</td>\n",
       "      <td>0.412755</td>\n",
       "      <td>0.412755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>5</td>\n",
       "      <td>0.028955</td>\n",
       "      <td>0.541742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.345754</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.541742</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.412700</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525889</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>acanthaluteres_vittiger</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.524712</td>\n",
       "      <td>0.142928</td>\n",
       "      <td>0.072555</td>\n",
       "      <td>0.031611</td>\n",
       "      <td>0.205233</td>\n",
       "      <td>0.026915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388147</td>\n",
       "      <td>0.508349</td>\n",
       "      <td>0.256860</td>\n",
       "      <td>0.236695</td>\n",
       "      <td>0.062374</td>\n",
       "      <td>0.097632</td>\n",
       "      <td>0.093562</td>\n",
       "      <td>0.286982</td>\n",
       "      <td>0.506582</td>\n",
       "      <td>0.508349</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 4765 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0                              1     2         3         4         5     \\\n",
       "0      1                       A73EGS-P     1  0.039008  0.000000  0.000000   \n",
       "1      2                       A73EGS-P     2  0.027035  0.000000  0.000000   \n",
       "2      3                       A73EGS-P     3  0.025006  0.000000  0.000000   \n",
       "3      4                       A73EGS-P     4  0.035968  0.414215  0.414215   \n",
       "4      5                       A73EGS-P     5  0.027966  0.000000  0.000000   \n",
       "5      6                       A73EGS-P     6  0.032001  0.401761  0.125557   \n",
       "6      7                       A73EGS-P     7  0.035001  0.000000  0.000000   \n",
       "7      8         acanthaluteres_brownii     1  0.025000  0.446425  0.411431   \n",
       "8      9         acanthaluteres_brownii     2  0.033000  0.444278  0.316934   \n",
       "9     10         acanthaluteres_brownii     3  0.029996  0.342638  0.342638   \n",
       "10    11         acanthaluteres_brownii     4  0.033960  0.528693  0.528693   \n",
       "11    12  acanthaluteres_spilomelanurus     1  0.025006  0.000000  0.000000   \n",
       "12    13  acanthaluteres_spilomelanurus     2  0.035000  0.657971  0.064249   \n",
       "13    14  acanthaluteres_spilomelanurus     3  0.030999  0.525479  0.211525   \n",
       "14    15        acanthaluteres_vittiger     1  0.026039  0.427143  0.427143   \n",
       "15    16        acanthaluteres_vittiger     2  0.027000  0.000000  0.000000   \n",
       "16    17        acanthaluteres_vittiger     3  0.034005  0.493773  0.451036   \n",
       "17    18        acanthaluteres_vittiger     4  0.030978  0.361551  0.344566   \n",
       "18    19        acanthaluteres_vittiger     5  0.028955  0.541742  0.000000   \n",
       "19    20        acanthaluteres_vittiger     6  0.033003  0.524712  0.142928   \n",
       "\n",
       "        6         7         8         9     ...      4755      4756      4757  \\\n",
       "0   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "1   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.402458  0.252968   \n",
       "2   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "3   0.414215  0.351986  0.414215  0.120783  ...  0.082304  0.432131  0.430995   \n",
       "4   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "5   0.401761  0.125557  0.401761  0.401761  ...  0.000000  0.526888  0.000000   \n",
       "6   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "7   0.446425  0.235103  0.446425  0.083122  ...  0.386499  0.376669  0.144562   \n",
       "8   0.392861  0.056470  0.444278  0.112940  ...  0.000000  0.601388  0.199759   \n",
       "9   0.246561  0.342638  0.342638  0.342638  ...  0.000000  0.000000  0.000000   \n",
       "10  0.166346  0.050066  0.191052  0.060581  ...  0.338371  0.501302  0.501302   \n",
       "11  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "12  0.016557  0.024136  0.137567  0.025158  ...  0.687689  0.350994  0.350994   \n",
       "13  0.267561  0.072952  0.310819  0.090654  ...  0.689251  0.510947  0.470526   \n",
       "14  0.427143  0.235712  0.251186  0.131000  ...  0.563029  0.560311  0.432229   \n",
       "15  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000  0.000000   \n",
       "16  0.240283  0.084106  0.180309  0.068568  ...  0.148360  0.502288  0.286801   \n",
       "17  0.240462  0.197831  0.361551  0.361551  ...  0.385947  0.412755  0.412755   \n",
       "18  0.345754  0.000000  0.541742  0.000000  ...  0.000000  0.525889  0.000000   \n",
       "19  0.072555  0.031611  0.205233  0.026915  ...  0.388147  0.508349  0.256860   \n",
       "\n",
       "        4758      4759      4760      4761      4762      4763      4764  \n",
       "0   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "1   0.402458  0.301150  0.402458  0.036988  0.329734  0.295521  0.402458  \n",
       "2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "3   0.432131  0.112794  0.432131  0.186883  0.432131  0.046721  0.132146  \n",
       "4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "5   0.456013  0.120170  0.526888  0.120170  0.456013  0.000000  0.000000  \n",
       "6   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "7   0.327824  0.376669  0.156876  0.376669  0.370978  0.376669  0.376669  \n",
       "8   0.084226  0.000000  0.238227  0.066586  0.601388  0.399518  0.094167  \n",
       "9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "10  0.282239  0.127177  0.158951  0.064110  0.319059  0.138134  0.501302  \n",
       "11  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "12  0.350994  0.350994  0.250163  0.273932  0.350994  0.350994  0.350994  \n",
       "13  0.101826  0.161002  0.187206  0.032200  0.183288  0.510947  0.387949  \n",
       "14  0.130764  0.038116  0.020357  0.016429  0.010390  0.407353  0.560311  \n",
       "15  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "16  0.129382  0.239781  0.102359  0.107700  0.254075  0.502288  0.502288  \n",
       "17  0.292546  0.153693  0.134140  0.190870  0.393578  0.412755  0.412755  \n",
       "18  0.412700  0.000000  0.525889  0.000000  0.525889  0.000000  0.000000  \n",
       "19  0.236695  0.062374  0.097632  0.093562  0.286982  0.506582  0.508349  \n",
       "\n",
       "[20 rows x 4765 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "METHOD_INDEX = 4\n",
    "# df = pd.read_csv(os.path.join(DATA_LBP_PATH,METHODS[METHOD_INDEX],METHODS[METHOD_INDEX]+'_dense_radius_is_3.csv'), header=None)\n",
    "df = pd.read_csv(os.path.join(DATA_HOG_PATH,'hog_fish.csv'), header=None)\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "lb = preprocessing.LabelEncoder()\n",
    "lb.fit(df[1].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 483, 483, 483])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_class = lb.transform(df[1])\n",
    "new_class\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_input =[i for i in range(4,260)]\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[index_input].copy().to_numpy(), new_class, \n",
    "                                                    test_size=0.333,random_state=1,stratify=new_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([348, 144, 417, ..., 460,  41, 100])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tr\n",
    "import torch.nn as tr_nn\n",
    "import torch.nn.functional as tr_fn\n",
    "import torch.optim as tr_optim\n",
    "# import torchvision\n",
    "# import torchvision.transforms as tr_transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "if(tr.cuda.is_available()):\n",
    "    device = tr.device('cuda')    \n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1471, 256])\n",
      "tensor([[0.5473, 0.0000, 0.5473,  ..., 0.4798, 0.2425, 0.0767],\n",
      "        [0.6837, 0.1666, 0.0329,  ..., 0.5609, 0.0916, 0.1278],\n",
      "        [0.4688, 0.2221, 0.4688,  ..., 0.3865, 0.3112, 0.3460],\n",
      "        ...,\n",
      "        [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "        [0.5203, 0.1312, 0.5203,  ..., 0.1293, 0.5724, 0.0000],\n",
      "        [0.4383, 0.3574, 0.3032,  ..., 0.3844, 0.3844, 0.3128]])\n"
     ]
    }
   ],
   "source": [
    "x_test_tr = tr.from_numpy(X_test).view(X_test.shape[0],256).type(tr.float)\n",
    "x_train_tr = tr.from_numpy(X_train).view(X_train.shape[0],256).type(tr.float)\n",
    "y_test_tr = tr.from_numpy(y_test).view(-1,1).type(tr.float)\n",
    "y_train_tr = tr.from_numpy(y_train).view(-1,1).type(tr.float)\n",
    "# print(x_train_tr.shape, y_train_tr)\n",
    "\n",
    "# x_test_tr = tr.randn(X_test.shape[0], 20)\n",
    "# x_train_tr = tr.randn(X_train.shape[0], 20)\n",
    "# y_test_tr = tr.from_numpy(y_test).view(-1,1).type(tr.float)\n",
    "# y_train_tr = tr.from_numpy(y_train).view(-1,1).type(tr.float)\n",
    "print(x_test_tr.shape)\n",
    "print(x_test_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.813333333333333\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "batch_size = 300\n",
    "rows_size = X_train.shape[0]\n",
    "batch_rows = rows_size/batch_size\n",
    "print(batch_rows)\n",
    "batch_epoch = math.ceil(batch_rows)\n",
    "print(batch_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(tr_nn.Module):\n",
    "    def __init__(self,input,output):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = tr_nn.Linear(input, 5000)\n",
    "        self.pool = tr_nn.MaxPool1d(3, 1)\n",
    "        self.conv2 = tr_nn.Linear(4998, 2000)\n",
    "        self.fc1 = tr_nn.Linear(2000, 1020)\n",
    "        self.fc2 = tr_nn.Linear(1020, 600)\n",
    "        self.fc3 = tr_nn.Linear(600, output)\n",
    "        self.dp = tr_nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.conv2(x)\n",
    "        x = self.dp(x)\n",
    "        # x = tr.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = tr_fn.relu(self.fc1(x))\n",
    "        x = self.dp(x)\n",
    "        x = tr_fn.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Linear(in_features=256, out_features=5000, bias=True)\n",
       "  (pool): MaxPool1d(kernel_size=3, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Linear(in_features=4998, out_features=2000, bias=True)\n",
       "  (fc1): Linear(in_features=2000, out_features=1020, bias=True)\n",
       "  (fc2): Linear(in_features=1020, out_features=600, bias=True)\n",
       "  (fc3): Linear(in_features=600, out_features=484, bias=True)\n",
       "  (dp): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(x_test_tr.shape[1],len(df[1].unique())).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# criterion = tr_nn.BCELoss()\n",
    "# optimizer = tr_optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = tr_nn.CrossEntropyLoss()\n",
    "optimizer = tr_optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 6.181760\n",
      "[1,     2] loss: 6.194138\n",
      "[1,     3] loss: 6.224454\n",
      "[1,     4] loss: 6.223843\n",
      "[1,     5] loss: 6.176038\n",
      "[1,     6] loss: 6.150723\n",
      "[1,     7] loss: 6.170039\n",
      "[1,     8] loss: 6.141735\n",
      "[1,     9] loss: 6.139662\n",
      "[1,    10] loss: 6.171342\n",
      "[1] acc 0.008\n",
      "[11,     1] loss: 4.136279\n",
      "[11,     2] loss: 3.795315\n",
      "[11,     3] loss: 3.975297\n",
      "[11,     4] loss: 4.253778\n",
      "[11,     5] loss: 3.984774\n",
      "[11,     6] loss: 4.137417\n",
      "[11,     7] loss: 4.105026\n",
      "[11,     8] loss: 3.874306\n",
      "[11,     9] loss: 3.843989\n",
      "[11,    10] loss: 3.826778\n",
      "[11] acc 0.052\n",
      "[21,     1] loss: 2.407529\n",
      "[21,     2] loss: 2.422088\n",
      "[21,     3] loss: 2.306245\n",
      "[21,     4] loss: 2.181573\n",
      "[21,     5] loss: 2.458708\n",
      "[21,     6] loss: 2.278996\n",
      "[21,     7] loss: 2.400770\n",
      "[21,     8] loss: 2.225919\n",
      "[21,     9] loss: 2.039299\n",
      "[21,    10] loss: 1.963828\n",
      "[21] acc 0.061\n",
      "[31,     1] loss: 0.929424\n",
      "[31,     2] loss: 1.048585\n",
      "[31,     3] loss: 0.774660\n",
      "[31,     4] loss: 0.818112\n",
      "[31,     5] loss: 0.748497\n",
      "[31,     6] loss: 0.965057\n",
      "[31,     7] loss: 0.920923\n",
      "[31,     8] loss: 0.967155\n",
      "[31,     9] loss: 0.832801\n",
      "[31,    10] loss: 0.581716\n",
      "[31] acc 0.068\n",
      "[41,     1] loss: 0.307454\n",
      "[41,     2] loss: 0.334598\n",
      "[41,     3] loss: 0.418606\n",
      "[41,     4] loss: 0.481067\n",
      "[41,     5] loss: 0.578245\n",
      "[41,     6] loss: 0.515450\n",
      "[41,     7] loss: 0.335755\n",
      "[41,     8] loss: 0.295977\n",
      "[41,     9] loss: 0.343190\n",
      "[41,    10] loss: 0.296092\n",
      "[41] acc 0.078\n",
      "[51,     1] loss: 0.234558\n",
      "[51,     2] loss: 0.216237\n",
      "[51,     3] loss: 0.240993\n",
      "[51,     4] loss: 0.270992\n",
      "[51,     5] loss: 0.154162\n",
      "[51,     6] loss: 0.146772\n",
      "[51,     7] loss: 0.101152\n",
      "[51,     8] loss: 0.121071\n",
      "[51,     9] loss: 0.214573\n",
      "[51,    10] loss: 0.133253\n",
      "[51] acc 0.091\n",
      "[61,     1] loss: 0.177407\n",
      "[61,     2] loss: 0.111077\n",
      "[61,     3] loss: 0.072065\n",
      "[61,     4] loss: 0.100957\n",
      "[61,     5] loss: 0.098801\n",
      "[61,     6] loss: 0.126783\n",
      "[61,     7] loss: 0.090742\n",
      "[61,     8] loss: 0.070783\n",
      "[61,     9] loss: 0.063990\n",
      "[61,    10] loss: 0.079399\n",
      "[61] acc 0.081\n",
      "[71,     1] loss: 0.040807\n",
      "[71,     2] loss: 0.028132\n",
      "[71,     3] loss: 0.037291\n",
      "[71,     4] loss: 0.048350\n",
      "[71,     5] loss: 0.018247\n",
      "[71,     6] loss: 0.028321\n",
      "[71,     7] loss: 0.041030\n",
      "[71,     8] loss: 0.029414\n",
      "[71,     9] loss: 0.037313\n",
      "[71,    10] loss: 0.017930\n",
      "[71] acc 0.094\n",
      "[81,     1] loss: 0.037202\n",
      "[81,     2] loss: 0.025319\n",
      "[81,     3] loss: 0.017784\n",
      "[81,     4] loss: 0.040809\n",
      "[81,     5] loss: 0.013513\n",
      "[81,     6] loss: 0.024229\n",
      "[81,     7] loss: 0.027356\n",
      "[81,     8] loss: 0.047818\n",
      "[81,     9] loss: 0.035276\n",
      "[81,    10] loss: 0.013081\n",
      "[81] acc 0.092\n",
      "[91,     1] loss: 0.036385\n",
      "[91,     2] loss: 0.010994\n",
      "[91,     3] loss: 0.012709\n",
      "[91,     4] loss: 0.033482\n",
      "[91,     5] loss: 0.011672\n",
      "[91,     6] loss: 0.023395\n",
      "[91,     7] loss: 0.019076\n",
      "[91,     8] loss: 0.021132\n",
      "[91,     9] loss: 0.033932\n",
      "[91,    10] loss: 0.012560\n",
      "[91] acc 0.097\n",
      "[101,     1] loss: 0.052228\n",
      "[101,     2] loss: 0.020207\n",
      "[101,     3] loss: 0.053403\n",
      "[101,     4] loss: 0.040085\n",
      "[101,     5] loss: 0.013162\n",
      "[101,     6] loss: 0.026810\n",
      "[101,     7] loss: 0.026317\n",
      "[101,     8] loss: 0.036311\n",
      "[101,     9] loss: 0.043887\n",
      "[101,    10] loss: 0.018462\n",
      "[101] acc 0.081\n",
      "[111,     1] loss: 0.059224\n",
      "[111,     2] loss: 0.043029\n",
      "[111,     3] loss: 0.065438\n",
      "[111,     4] loss: 0.171924\n",
      "[111,     5] loss: 0.058915\n",
      "[111,     6] loss: 0.112889\n",
      "[111,     7] loss: 0.057762\n",
      "[111,     8] loss: 0.077776\n",
      "[111,     9] loss: 0.062423\n",
      "[111,    10] loss: 0.122400\n",
      "[111] acc 0.083\n",
      "[121,     1] loss: 0.194971\n",
      "[121,     2] loss: 0.112384\n",
      "[121,     3] loss: 0.089212\n",
      "[121,     4] loss: 0.146146\n",
      "[121,     5] loss: 0.048743\n",
      "[121,     6] loss: 0.176121\n",
      "[121,     7] loss: 0.093215\n",
      "[121,     8] loss: 0.066594\n",
      "[121,     9] loss: 0.138347\n",
      "[121,    10] loss: 0.058897\n",
      "[121] acc 0.084\n",
      "[131,     1] loss: 0.213475\n",
      "[131,     2] loss: 0.322718\n",
      "[131,     3] loss: 0.148996\n",
      "[131,     4] loss: 0.275517\n",
      "[131,     5] loss: 0.196257\n",
      "[131,     6] loss: 0.261793\n",
      "[131,     7] loss: 0.230898\n",
      "[131,     8] loss: 0.231664\n",
      "[131,     9] loss: 0.256138\n",
      "[131,    10] loss: 0.222150\n",
      "[131] acc 0.081\n",
      "[141,     1] loss: 0.155029\n",
      "[141,     2] loss: 0.149625\n",
      "[141,     3] loss: 0.161600\n",
      "[141,     4] loss: 0.158352\n",
      "[141,     5] loss: 0.408011\n",
      "[141,     6] loss: 0.308094\n",
      "[141,     7] loss: 0.232510\n",
      "[141,     8] loss: 0.238750\n",
      "[141,     9] loss: 0.187903\n",
      "[141,    10] loss: 0.198021\n",
      "[141] acc 0.077\n",
      "[151,     1] loss: 0.362998\n",
      "[151,     2] loss: 0.233735\n",
      "[151,     3] loss: 0.281406\n",
      "[151,     4] loss: 0.343585\n",
      "[151,     5] loss: 0.206699\n",
      "[151,     6] loss: 0.375127\n",
      "[151,     7] loss: 0.259701\n",
      "[151,     8] loss: 0.272124\n",
      "[151,     9] loss: 0.196876\n",
      "[151,    10] loss: 0.271155\n",
      "[151] acc 0.080\n",
      "[161,     1] loss: 0.397475\n",
      "[161,     2] loss: 0.232174\n",
      "[161,     3] loss: 0.209377\n",
      "[161,     4] loss: 0.356316\n",
      "[161,     5] loss: 0.270450\n",
      "[161,     6] loss: 0.316288\n",
      "[161,     7] loss: 0.245187\n",
      "[161,     8] loss: 0.218418\n",
      "[161,     9] loss: 0.306870\n",
      "[161,    10] loss: 0.144507\n",
      "[161] acc 0.067\n",
      "[171,     1] loss: 0.262480\n",
      "[171,     2] loss: 0.388300\n",
      "[171,     3] loss: 0.248179\n",
      "[171,     4] loss: 0.194965\n",
      "[171,     5] loss: 0.222475\n",
      "[171,     6] loss: 0.313486\n",
      "[171,     7] loss: 0.305628\n",
      "[171,     8] loss: 0.315619\n",
      "[171,     9] loss: 0.518518\n",
      "[171,    10] loss: 0.279570\n",
      "[171] acc 0.077\n",
      "[181,     1] loss: 0.640057\n",
      "[181,     2] loss: 0.459622\n",
      "[181,     3] loss: 0.591011\n",
      "[181,     4] loss: 0.449412\n",
      "[181,     5] loss: 0.370607\n",
      "[181,     6] loss: 0.290163\n",
      "[181,     7] loss: 0.282243\n",
      "[181,     8] loss: 0.209076\n",
      "[181,     9] loss: 0.356888\n",
      "[181,    10] loss: 0.222648\n",
      "[181] acc 0.075\n",
      "[191,     1] loss: 0.408550\n",
      "[191,     2] loss: 0.088004\n",
      "[191,     3] loss: 0.409978\n",
      "[191,     4] loss: 0.073108\n",
      "[191,     5] loss: 0.173772\n",
      "[191,     6] loss: 0.130588\n",
      "[191,     7] loss: 0.356710\n",
      "[191,     8] loss: 0.293660\n",
      "[191,     9] loss: 0.192172\n",
      "[191,    10] loss: 0.057602\n",
      "[191] acc 0.073\n",
      "[201,     1] loss: 0.079814\n",
      "[201,     2] loss: 0.119151\n",
      "[201,     3] loss: 0.177963\n",
      "[201,     4] loss: 0.154244\n",
      "[201,     5] loss: 0.122904\n",
      "[201,     6] loss: 0.114794\n",
      "[201,     7] loss: 0.099936\n",
      "[201,     8] loss: 0.144406\n",
      "[201,     9] loss: 0.189141\n",
      "[201,    10] loss: 0.244943\n",
      "[201] acc 0.067\n",
      "[211,     1] loss: 0.237055\n",
      "[211,     2] loss: 0.116816\n",
      "[211,     3] loss: 0.202708\n",
      "[211,     4] loss: 0.258033\n",
      "[211,     5] loss: 0.074011\n",
      "[211,     6] loss: 0.192104\n",
      "[211,     7] loss: 0.282558\n",
      "[211,     8] loss: 0.159696\n",
      "[211,     9] loss: 0.127877\n",
      "[211,    10] loss: 0.175815\n",
      "[211] acc 0.080\n",
      "[221,     1] loss: 0.144356\n",
      "[221,     2] loss: 0.180750\n",
      "[221,     3] loss: 0.343840\n",
      "[221,     4] loss: 0.192059\n",
      "[221,     5] loss: 0.188384\n",
      "[221,     6] loss: 0.172898\n",
      "[221,     7] loss: 0.081255\n",
      "[221,     8] loss: 0.144937\n",
      "[221,     9] loss: 0.273377\n",
      "[221,    10] loss: 0.073381\n",
      "[221] acc 0.074\n",
      "[231,     1] loss: 0.400877\n",
      "[231,     2] loss: 0.122915\n",
      "[231,     3] loss: 0.391818\n",
      "[231,     4] loss: 0.197578\n",
      "[231,     5] loss: 0.104904\n",
      "[231,     6] loss: 0.090598\n",
      "[231,     7] loss: 0.342962\n",
      "[231,     8] loss: 0.323877\n",
      "[231,     9] loss: 0.187833\n",
      "[231,    10] loss: 0.303890\n",
      "[231] acc 0.072\n",
      "[241,     1] loss: 0.180551\n",
      "[241,     2] loss: 0.222286\n",
      "[241,     3] loss: 0.144796\n",
      "[241,     4] loss: 0.190608\n",
      "[241,     5] loss: 0.202737\n",
      "[241,     6] loss: 0.202328\n",
      "[241,     7] loss: 0.425435\n",
      "[241,     8] loss: 0.210774\n",
      "[241,     9] loss: 0.345475\n",
      "[241,    10] loss: 0.198025\n",
      "[241] acc 0.073\n",
      "[251,     1] loss: 0.306035\n",
      "[251,     2] loss: 0.522705\n",
      "[251,     3] loss: 0.111450\n",
      "[251,     4] loss: 0.240082\n",
      "[251,     5] loss: 0.489477\n",
      "[251,     6] loss: 0.302244\n",
      "[251,     7] loss: 0.070525\n",
      "[251,     8] loss: 0.442760\n",
      "[251,     9] loss: 0.555148\n",
      "[251,    10] loss: 0.377244\n",
      "[251] acc 0.077\n",
      "[261,     1] loss: 0.273321\n",
      "[261,     2] loss: 0.232753\n",
      "[261,     3] loss: 0.499387\n",
      "[261,     4] loss: 0.582943\n",
      "[261,     5] loss: 0.287084\n",
      "[261,     6] loss: 0.733513\n",
      "[261,     7] loss: 0.670568\n",
      "[261,     8] loss: 0.561683\n",
      "[261,     9] loss: 0.740688\n",
      "[261,    10] loss: 0.342114\n",
      "[261] acc 0.076\n",
      "[271,     1] loss: 0.171824\n",
      "[271,     2] loss: 0.120391\n",
      "[271,     3] loss: 0.178678\n",
      "[271,     4] loss: 0.266456\n",
      "[271,     5] loss: 0.092441\n",
      "[271,     6] loss: 0.424718\n",
      "[271,     7] loss: 0.108121\n",
      "[271,     8] loss: 0.216755\n",
      "[271,     9] loss: 0.335741\n",
      "[271,    10] loss: 0.390933\n",
      "[271] acc 0.073\n",
      "[281,     1] loss: 0.245093\n",
      "[281,     2] loss: 0.239633\n",
      "[281,     3] loss: 0.086565\n",
      "[281,     4] loss: 0.153831\n",
      "[281,     5] loss: 0.092950\n",
      "[281,     6] loss: 0.186433\n",
      "[281,     7] loss: 0.236613\n",
      "[281,     8] loss: 0.315360\n",
      "[281,     9] loss: 0.225438\n",
      "[281,    10] loss: 0.064537\n",
      "[281] acc 0.081\n",
      "[291,     1] loss: 0.551057\n",
      "[291,     2] loss: 1.306733\n",
      "[291,     3] loss: 0.214205\n",
      "[291,     4] loss: 0.229445\n",
      "[291,     5] loss: 0.462971\n",
      "[291,     6] loss: 0.671292\n",
      "[291,     7] loss: 0.426957\n",
      "[291,     8] loss: 0.223066\n",
      "[291,     9] loss: 0.526750\n",
      "[291,    10] loss: 0.728261\n",
      "[291] acc 0.068\n",
      "[301,     1] loss: 0.123868\n",
      "[301,     2] loss: 1.070705\n",
      "[301,     3] loss: 0.799579\n",
      "[301,     4] loss: 0.765634\n",
      "[301,     5] loss: 0.873081\n",
      "[301,     6] loss: 0.890540\n",
      "[301,     7] loss: 0.777880\n",
      "[301,     8] loss: 0.308479\n",
      "[301,     9] loss: 0.327599\n",
      "[301,    10] loss: 0.095008\n",
      "[301] acc 0.075\n",
      "[311,     1] loss: 0.610204\n",
      "[311,     2] loss: 0.573650\n",
      "[311,     3] loss: 0.145394\n",
      "[311,     4] loss: 0.268332\n",
      "[311,     5] loss: 0.462508\n",
      "[311,     6] loss: 0.287684\n",
      "[311,     7] loss: 0.383892\n",
      "[311,     8] loss: 0.412567\n",
      "[311,     9] loss: 0.560079\n",
      "[311,    10] loss: 0.424033\n",
      "[311] acc 0.076\n",
      "[321,     1] loss: 0.384735\n",
      "[321,     2] loss: 0.402908\n",
      "[321,     3] loss: 0.492032\n",
      "[321,     4] loss: 0.486200\n",
      "[321,     5] loss: 0.242038\n",
      "[321,     6] loss: 0.423664\n",
      "[321,     7] loss: 0.068118\n",
      "[321,     8] loss: 0.278544\n",
      "[321,     9] loss: 0.832219\n",
      "[321,    10] loss: 0.491151\n",
      "[321] acc 0.086\n",
      "[331,     1] loss: 0.541101\n",
      "[331,     2] loss: 0.521884\n",
      "[331,     3] loss: 0.353071\n",
      "[331,     4] loss: 0.110042\n",
      "[331,     5] loss: 0.125295\n",
      "[331,     6] loss: 0.432286\n",
      "[331,     7] loss: 0.229640\n",
      "[331,     8] loss: 0.220085\n",
      "[331,     9] loss: 0.367493\n",
      "[331,    10] loss: 0.107259\n",
      "[331] acc 0.084\n",
      "[341,     1] loss: 0.326932\n",
      "[341,     2] loss: 0.565422\n",
      "[341,     3] loss: 0.348021\n",
      "[341,     4] loss: 0.413085\n",
      "[341,     5] loss: 0.406384\n",
      "[341,     6] loss: 0.626381\n",
      "[341,     7] loss: 0.217827\n",
      "[341,     8] loss: 0.261439\n",
      "[341,     9] loss: 0.328626\n",
      "[341,    10] loss: 0.345819\n",
      "[341] acc 0.073\n",
      "[351,     1] loss: 0.219596\n",
      "[351,     2] loss: 0.365040\n",
      "[351,     3] loss: 0.365239\n",
      "[351,     4] loss: 0.147068\n",
      "[351,     5] loss: 0.391710\n",
      "[351,     6] loss: 0.587118\n",
      "[351,     7] loss: 0.489912\n",
      "[351,     8] loss: 0.376436\n",
      "[351,     9] loss: 0.520948\n",
      "[351,    10] loss: 0.147820\n",
      "[351] acc 0.071\n",
      "[361,     1] loss: 0.380675\n",
      "[361,     2] loss: 0.288716\n",
      "[361,     3] loss: 0.197476\n",
      "[361,     4] loss: 0.223400\n",
      "[361,     5] loss: 0.251172\n",
      "[361,     6] loss: 0.184149\n",
      "[361,     7] loss: 0.413070\n",
      "[361,     8] loss: 0.167536\n",
      "[361,     9] loss: 0.112009\n",
      "[361,    10] loss: 0.014670\n",
      "[361] acc 0.059\n",
      "[371,     1] loss: 0.180972\n",
      "[371,     2] loss: 0.341214\n",
      "[371,     3] loss: 0.279117\n",
      "[371,     4] loss: 1.023490\n",
      "[371,     5] loss: 0.215848\n",
      "[371,     6] loss: 0.065520\n",
      "[371,     7] loss: 0.052810\n",
      "[371,     8] loss: 0.315826\n",
      "[371,     9] loss: 0.242126\n",
      "[371,    10] loss: 0.102397\n",
      "[371] acc 0.076\n",
      "[381,     1] loss: 0.603836\n",
      "[381,     2] loss: 0.150405\n",
      "[381,     3] loss: 0.129099\n",
      "[381,     4] loss: 0.297432\n",
      "[381,     5] loss: 0.680944\n",
      "[381,     6] loss: 0.518219\n",
      "[381,     7] loss: 0.091153\n",
      "[381,     8] loss: 0.285818\n",
      "[381,     9] loss: 0.547234\n",
      "[381,    10] loss: 0.201971\n",
      "[381] acc 0.067\n",
      "[391,     1] loss: 0.884369\n",
      "[391,     2] loss: 0.519092\n",
      "[391,     3] loss: 0.861988\n",
      "[391,     4] loss: 0.880853\n",
      "[391,     5] loss: 0.879144\n",
      "[391,     6] loss: 1.259565\n",
      "[391,     7] loss: 0.766684\n",
      "[391,     8] loss: 0.364868\n",
      "[391,     9] loss: 0.521400\n",
      "[391,    10] loss: 0.598731\n",
      "[391] acc 0.077\n",
      "[401,     1] loss: 0.361919\n",
      "[401,     2] loss: 0.558146\n",
      "[401,     3] loss: 0.161030\n",
      "[401,     4] loss: 0.401173\n",
      "[401,     5] loss: 0.068680\n",
      "[401,     6] loss: 0.359917\n",
      "[401,     7] loss: 0.205576\n",
      "[401,     8] loss: 0.565040\n",
      "[401,     9] loss: 0.549944\n",
      "[401,    10] loss: 0.014769\n",
      "[401] acc 0.077\n",
      "[411,     1] loss: 0.111530\n",
      "[411,     2] loss: 0.539796\n",
      "[411,     3] loss: 0.515738\n",
      "[411,     4] loss: 0.502877\n",
      "[411,     5] loss: 0.471193\n",
      "[411,     6] loss: 0.421084\n",
      "[411,     7] loss: 0.593164\n",
      "[411,     8] loss: 1.266460\n",
      "[411,     9] loss: 0.318253\n",
      "[411,    10] loss: 0.171859\n",
      "[411] acc 0.075\n",
      "[421,     1] loss: 0.233892\n",
      "[421,     2] loss: 0.613768\n",
      "[421,     3] loss: 0.605704\n",
      "[421,     4] loss: 0.380870\n",
      "[421,     5] loss: 0.321579\n",
      "[421,     6] loss: 0.587584\n",
      "[421,     7] loss: 0.086603\n",
      "[421,     8] loss: 0.286842\n",
      "[421,     9] loss: 0.721346\n",
      "[421,    10] loss: 0.128539\n",
      "[421] acc 0.073\n",
      "[431,     1] loss: 0.297518\n",
      "[431,     2] loss: 0.844609\n",
      "[431,     3] loss: 0.088236\n",
      "[431,     4] loss: 0.124852\n",
      "[431,     5] loss: 0.187257\n",
      "[431,     6] loss: 0.173000\n",
      "[431,     7] loss: 0.090475\n",
      "[431,     8] loss: 0.373570\n",
      "[431,     9] loss: 0.211501\n",
      "[431,    10] loss: 0.543218\n",
      "[431] acc 0.074\n",
      "[441,     1] loss: 0.483371\n",
      "[441,     2] loss: 0.459480\n",
      "[441,     3] loss: 0.480269\n",
      "[441,     4] loss: 0.133289\n",
      "[441,     5] loss: 0.049762\n",
      "[441,     6] loss: 0.502347\n",
      "[441,     7] loss: 0.268199\n",
      "[441,     8] loss: 0.616600\n",
      "[441,     9] loss: 0.252943\n",
      "[441,    10] loss: 0.202339\n",
      "[441] acc 0.075\n",
      "[451,     1] loss: 0.740475\n",
      "[451,     2] loss: 0.664607\n",
      "[451,     3] loss: 1.090048\n",
      "[451,     4] loss: 1.009327\n",
      "[451,     5] loss: 0.605505\n",
      "[451,     6] loss: 0.543916\n",
      "[451,     7] loss: 0.473761\n",
      "[451,     8] loss: 1.148876\n",
      "[451,     9] loss: 1.583285\n",
      "[451,    10] loss: 0.781558\n",
      "[451] acc 0.084\n",
      "[461,     1] loss: 1.318850\n",
      "[461,     2] loss: 0.414939\n",
      "[461,     3] loss: 0.494634\n",
      "[461,     4] loss: 0.351660\n",
      "[461,     5] loss: 0.299190\n",
      "[461,     6] loss: 0.294863\n",
      "[461,     7] loss: 0.164921\n",
      "[461,     8] loss: 0.241186\n",
      "[461,     9] loss: 0.397061\n",
      "[461,    10] loss: 0.391802\n",
      "[461] acc 0.075\n",
      "[471,     1] loss: 0.270219\n",
      "[471,     2] loss: 0.232912\n",
      "[471,     3] loss: 0.250022\n",
      "[471,     4] loss: 0.340567\n",
      "[471,     5] loss: 0.723566\n",
      "[471,     6] loss: 0.614371\n",
      "[471,     7] loss: 0.539436\n",
      "[471,     8] loss: 0.136337\n",
      "[471,     9] loss: 0.129386\n",
      "[471,    10] loss: 0.299575\n",
      "[471] acc 0.077\n",
      "[481,     1] loss: 0.609641\n",
      "[481,     2] loss: 0.525447\n",
      "[481,     3] loss: 0.019991\n",
      "[481,     4] loss: 0.354539\n",
      "[481,     5] loss: 0.431662\n",
      "[481,     6] loss: 0.415711\n",
      "[481,     7] loss: 0.095992\n",
      "[481,     8] loss: 0.482158\n",
      "[481,     9] loss: 0.110753\n",
      "[481,    10] loss: 0.127750\n",
      "[481] acc 0.078\n",
      "[491,     1] loss: 0.752808\n",
      "[491,     2] loss: 0.558675\n",
      "[491,     3] loss: 0.141938\n",
      "[491,     4] loss: 0.194812\n",
      "[491,     5] loss: 0.585157\n",
      "[491,     6] loss: 0.426053\n",
      "[491,     7] loss: 0.280134\n",
      "[491,     8] loss: 0.629061\n",
      "[491,     9] loss: 0.165697\n",
      "[491,    10] loss: 0.096376\n",
      "[491] acc 0.073\n",
      "[501,     1] loss: 0.605211\n",
      "[501,     2] loss: 0.380611\n",
      "[501,     3] loss: 0.120545\n",
      "[501,     4] loss: 0.794254\n",
      "[501,     5] loss: 0.280405\n",
      "[501,     6] loss: 0.689528\n",
      "[501,     7] loss: 0.325660\n",
      "[501,     8] loss: 0.227127\n",
      "[501,     9] loss: 0.856604\n",
      "[501,    10] loss: 0.758912\n",
      "[501] acc 0.082\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24108/3080171636.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;31m# # print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mrunning_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# print every 2000 mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.6f}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(2000):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    for i in range(0,batch_epoch):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        endp = (i+1)*batch_size\n",
    "        inputs = x_train_tr[i*batch_size:endp if endp < rows_size else rows_size,:]\n",
    "        labels = y_train_tr [i*batch_size:endp if endp < rows_size else rows_size]\n",
    "        labels = labels.to(device)\n",
    "        inputs = inputs.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # print(\"inuts\", inputs)\n",
    "        # print(\"labels\",labels)\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        # print(\"outputs\",outputs)\n",
    "        labels = labels.squeeze(1).type(tr.LongTensor).to(device)\n",
    "        # print(\"label seq\",labels)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if epoch % 10 == 0:    # print every 2000 mini-batches\n",
    "            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss:.6f}')\n",
    "        running_loss = 0.0\n",
    "    if epoch % 10 ==0:\n",
    "         with tr.no_grad():\n",
    "                y_test_tr_tr = y_test_tr.to(device).squeeze(1).type(tr.int)\n",
    "                y_pr = model(x_test_tr.to(device))\n",
    "                _, y_pr_class = tr.max(y_pr,1)\n",
    "                acc = (y_pr_class==y_test_tr_tr).sum().float()/float(y_test_tr_tr.shape[0])\n",
    "                print(f\"[{epoch + 1}] acc {acc:.3f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([78, 78, 78,  ..., 78, 78, 78], device='cuda:0') tensor([162,  63, 130,  ..., 157, 110, 158], device='cuda:0',\n",
      "       dtype=torch.int32)\n",
      "acc tensor(0.0027, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "with tr.no_grad():\n",
    "    y_test_tr_tr = y_test_tr.to(device).squeeze(1).type(tr.int)\n",
    "    y_pr = model(x_test_tr.to(device))\n",
    "    _, y_pr_class = tr.max(y_pr,1)\n",
    "    \n",
    "    print(y_pr_class,y_test_tr_tr)\n",
    "    acc = (y_pr_class==y_test_tr_tr).sum().float()/float(y_test_tr_tr.shape[0])\n",
    "    # for i in range(0,y_pr_class.shape[0]):\n",
    "    #     print(y_pr_class[i],y_test_tr[i])\n",
    "    print(\"acc\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected 3-dimensional input for 3-dimensional weight [200, 256, 1], but got 2-dimensional input of size [0, 20] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21964/1337557945.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# print(m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0myyy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_test_tr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myyy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21964/1337557945.py\u001b[0m in \u001b[0;36mtt\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtr_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# flatten all dimensions except batch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 301\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    295\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m                             _single(0), self.dilation, self.groups)\n\u001b[1;32m--> 297\u001b[1;33m         return F.conv1d(input, weight, bias, self.stride,\n\u001b[0m\u001b[0;32m    298\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected 3-dimensional input for 3-dimensional weight [200, 256, 1], but got 2-dimensional input of size [0, 20] instead"
     ]
    }
   ],
   "source": [
    "conv1 = tr_nn.Conv1d(256, 200, 1)\n",
    "pool = tr_nn.MaxPool1d(1)\n",
    "conv2 = tr_nn.Conv1d(200, 16, 1)\n",
    "fc1 = tr_nn.Linear(16, 120)\n",
    "fc2 = tr_nn.Linear(120, 84)\n",
    "fc3 = tr_nn.Linear(84, 10)\n",
    "\n",
    "def tt(x):\n",
    "    x = pool(tr_fn.relu(conv1(x)))\n",
    "    x = pool(tr_fn.relu(conv2(x)))\n",
    "    x = tr.flatten(x, 1) # flatten all dimensions except batch\n",
    "    x = tr_fn.relu(fc1(x))\n",
    "    x = tr_fn.relu(fc2(x))\n",
    "    x = fc3(x)\n",
    "    return x\n",
    "# input = tr.randn(20, 15, 50)\n",
    "# print(input[2].shape)\n",
    "# print(m)\n",
    "yyy = x_test_tr[i*batch_size:(i+1)*batch_size,:]\n",
    "output = tt(yyy)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
